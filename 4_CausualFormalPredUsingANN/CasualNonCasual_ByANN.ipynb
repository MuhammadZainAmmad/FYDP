{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file\n",
    "df = pd.read_csv(\"../2_SeparatingMaleDataset/maleDS.csv\")\n",
    "\n",
    "# Directory of the images\n",
    "src_path = \"../2_SeparatingMaleDataset/maleDS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "defectedImagesIDs = []\n",
    "goodImagesIDs = []\n",
    "folderPathOfImage = src_path\n",
    "\n",
    "for id in df['id']:\n",
    "    imagePathVariable = folderPathOfImage + str(id)+'.jpg'\n",
    "    image = Image.open(imagePathVariable)\n",
    "    numpyArrayImg = np.array(image)\n",
    "        \n",
    "    a = (numpyArrayImg.shape)  \n",
    "    if a == (80, 60, 3):\n",
    "        goodImagesIDs.append(id)\n",
    "    else:\n",
    "        defectedImagesIDs.append(id)\n",
    "        df.drop(df[df['id'] == id].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24781\n",
      "299\n",
      "25080\n"
     ]
    }
   ],
   "source": [
    "print(len(goodImagesIDs))\n",
    "print(len(defectedImagesIDs))\n",
    "print(len(goodImagesIDs) + len(defectedImagesIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    24761\n",
       "gender                24761\n",
       "masterCategory        24761\n",
       "subCategory           24761\n",
       "articleType           24761\n",
       "baseColour            24761\n",
       "season                24761\n",
       "year                  24761\n",
       "usage                 24761\n",
       "productDisplayName    24761\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumpyArrays = []\n",
    "for i in goodImagesIDs:\n",
    "    imagePathVariable = folderPathOfImage + str(i)+'.jpg'\n",
    "    image = Image.open(imagePathVariable)\n",
    "    numpyArrayImg = np.array(image)\n",
    "    NumpyArrays.append([numpyArrayImg])\n",
    "df_images = np.array(NumpyArrays)\n",
    "type(df_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24761,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the usage column into Casual and Formal\n",
    "df_usage = df['usage']\n",
    "df_usage.head()\n",
    "df_usage.shape # (24761,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24761,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining Lambda Function to encode the values of the column\n",
    "encoderFunction = lambda valueOfColumn: 1 if (valueOfColumn == 'Formal') else 0\n",
    "df_usageEncoded = np.vectorize(encoderFunction)(df_usage)\n",
    "print(type(df_usageEncoded))\n",
    "df_usageEncoded.shape # (24761,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formal count (1): 2205\n",
      "casual count (0): 22556\n"
     ]
    }
   ],
   "source": [
    "formalCount, casualCount = 0, 0\n",
    "for i in df_usageEncoded:\n",
    "    # print(i)\n",
    "    if i ==1:\n",
    "        formalCount += 1\n",
    "    elif i ==0:\n",
    "        casualCount += 1\n",
    "print(f'formal count (1): {formalCount}') # 2,205 i.e. correct number of 1s (formal)\n",
    "print(f'casual count (0): {casualCount}') # 22,556 i.e. correct number of 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now we have two numpy.ndarray\n",
    "1) df_images: contain numpy array of all images\n",
    "2) df_usageEncoded: contain label of each image in form of 0 and 1\n",
    "\n",
    "so df_images is x and df_usageEncoded is y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1788\\3805744594.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "print(tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2' has no attribute '__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1788\\3220186848.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# from tensorflow.keras.utils import array_to_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_to_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfenv\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfenv\\lib\\site-packages\\keras\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfenv\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayout_map\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfenv\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_coordinator_utils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfenv\\lib\\site-packages\\keras\\backend_config.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mkeras_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"keras.backend.epsilon\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \"\"\"Returns the value of the fuzz factor used in numeric expressions.\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2' has no attribute '__internal__'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "# from tensorflow.keras.utils import array_to_img\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from time import strftime # gives hours and minutes of current time.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and flattening the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data between 0 and 1 (Normalization)\n",
    "x_scaled = df_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.98823529, 1.        , 0.97254902],\n",
       "          [1.        , 1.        , 0.98039216],\n",
       "          [1.        , 0.98431373, 1.        ],\n",
       "          ...,\n",
       "          [0.99607843, 1.        , 1.        ],\n",
       "          [0.99607843, 1.        , 1.        ],\n",
       "          [1.        , 1.        , 0.99215686]],\n",
       "\n",
       "         [[0.99607843, 1.        , 0.96470588],\n",
       "          [1.        , 1.        , 0.98039216],\n",
       "          [1.        , 0.98431373, 1.        ],\n",
       "          ...,\n",
       "          [0.99607843, 1.        , 1.        ],\n",
       "          [0.99607843, 1.        , 1.        ],\n",
       "          [1.        , 1.        , 0.98431373]],\n",
       "\n",
       "         [[1.        , 1.        , 0.96470588],\n",
       "          [1.        , 0.99607843, 0.98039216],\n",
       "          [1.        , 0.98431373, 1.        ],\n",
       "          ...,\n",
       "          [0.99607843, 1.        , 1.        ],\n",
       "          [0.99607843, 1.        , 0.99215686],\n",
       "          [1.        , 1.        , 0.98431373]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]],\n",
       "\n",
       "         [[1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          ...,\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ],\n",
       "          [1.        , 1.        , 1.        ]]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us see the data again after scaling.\n",
    "x_scaled\n",
    "#note that this is a 4D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening the train tensor; placing all pixel for one image in one dimension\n",
    "TOTAL_INPUTS = 80*60*3\n",
    "x_scaled_flat = x_scaled.reshape(x_scaled.shape[0], TOTAL_INPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        0.98431373],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us see how this flat array looks like\n",
    "x_scaled_flat\n",
    "#Now it is a 2D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24761, 14400)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled_flat.shape\n",
    "#Note that 14400 = 80 x 60 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train, test and validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 14400)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_SIZE = 5000\n",
    "\n",
    "##Creating test set\n",
    "x_test = x_scaled_flat[:TEST_SIZE]\n",
    "y_test = df_usageEncoded[:TEST_SIZE]\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 14400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAL_SIZE = 1000\n",
    "\n",
    "##Creating test set\n",
    "x_val = x_scaled_flat[:VAL_SIZE]\n",
    "y_val = df_usageEncoded[:VAL_SIZE]\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18761, 14400)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Creating the remaining train set\n",
    "x_train = x_scaled_flat[TEST_SIZE + VAL_SIZE:]\n",
    "y_train = df_usageEncoded[TEST_SIZE + VAL_SIZE:]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have two scaled and flattened datasets:\n",
    "- The train set haing 18761 samples\n",
    "- The test set having 5000 samples\n",
    "- The validation set having 1000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential([\n",
    "    Dense(units=128, input_dim=TOTAL_INPUTS, activation='relu', name='m1_hidden1'),\n",
    "    Dense(units=64, activation='relu', name='m1_hidden2'),\n",
    "    Dense(16, activation='relu', name='m1_hidden3'),\n",
    "    Dense(10, activation='softmax', name='m1_output')\n",
    "])\n",
    "#if we donot give names to the layers, then the names keep on changing on every run\n",
    "\n",
    "model_1.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_1)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total neurons in layer m1_hidden1 = (TOTAL_INPUTS+1)*128 = ((80*60*3)+1)*128 = 1843328\n",
    "- Total neurons in layer m1_hidden2 = (128+1)*64  \n",
    "- Total neurons in layer m1_hidden3 = (64+1)*16\n",
    "- Total neurons in layer m1_output = (16+1)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard (visualising learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting main folder and subfolders for tendboard\n",
    "LOG_DIR = 'tensorboard_cifar_logs/'\n",
    "\n",
    "def get_tensorboard(model_name):\n",
    "    sub_folder_name = f'{model_name}_at_{strftime(\"%H_%M\")}'\n",
    "    dir_paths = os.path.join(LOG_DIR, sub_folder_name)\n",
    "    os.makedirs(dir_paths)\n",
    "    return TensorBoard(log_dir=dir_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tensor board in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=tensorboard_cifar_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_batch = 1000\n",
    "nr_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_1.fit(x_train, y_train, batch_size=samples_per_batch, epochs=nr_epochs,\n",
    "            callbacks=[get_tensorboard('Model_1')], verbose=0, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on Individual Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following code model_1 is used for prediction.\n",
    "- You may use model_2 or model_3 too by making necessary alterations in the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_nr=10\n",
    "x_val[image_nr].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding a dimension as per requirement of predict method\n",
    "test = np.expand_dims(x_val[image_nr], axis=0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking the highest probability class\n",
    "predicted_value=np.argmax(model_1.predict(test), axis=1)\n",
    "actual_value=y_val[image_nr]\n",
    "\n",
    "print(f'Actual value: {actual_value} vs. predicted: {predicted_value[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalling the metrics that we set during compilation of the model.\n",
    "model_1.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us print the loss funcstion value and overall accuracy of our model on test data.\n",
    "test_loss, test_accuracy = model_1.evaluate(x_test, y_test)\n",
    "print(f'Test loss is {test_loss:0.3} and test accuracy is {test_accuracy:0.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
